---
title: "Plotting"
author: "Sigrid Agersnap Bom Nielsen"
date: "11/15/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loading packages}
pacman::p_load(tidyverse, lubridate, lmerTest)
```

# The big plotting R markdown

```{r loading data}
# load data
d <- read_csv('df_full.csv')
```

# Inspecting data
Getting some stats of the data

```{r}
# investigating subsections
unique(d$subsection)
```

So, we have 8 subsections;
- Europe, Asia Pacific, Africa, Canada, Americas, Middle East, Australia and 'What in the World'. 

I'm not particularily interested in the category of 'What in the World', so I'll see if it contains a lot of data. 

```{r}
d %>% 
  filter(
    subsection == 'What in the World'
    ) %>% 
  count()
```
Since the 'What in the World' subsection only contains 2 datapoints, I'll exclude it from analysis. 
Also, by investigating NYT's webpage, it seems that it is a depricated category, which they do not update anymore. 

```{r}
# removing 'What in the World' from the dataset
d <- d %>% filter(
  subsection != 'What in the World'
)
```

Judging by the size of the dataframe, I have 3,490 articles. 

Now, I'll look into what kind of articles I'm dealing with. 

```{r}
#types of 'articles'
unique(d$type)
```

We have 6 different types; news, obituary, interactive feature, video, briefing and news analysis

I'd like to know how many words there is in each type of article on average. 

```{r}
################ Dealing with the different types of news ##########33
# plot mean of words in different types of 'articles'
d %>% 
  ggplot() +
  aes(type, word_count) + 
  stat_summary(fun = 'mean', geom = 'point') +
  stat_summary(fun.data = 'mean_se', geom = 'errorbar', width = .1) +
  labs(
    title = '',
    x = '',
    y = 'word count'
  )
```
From this know that the different types of news containing more text than a simple headline are 'briefing', 'News', 'News Analysis' and 'Obituary (Obit)'. 

```{r}
# counting number of 'articles' with no text
d %>% 
  filter(
    word_count == 0
  ) %>% 
  count()

# count number of non-text objects (only headlines)
d %>% 
  filter(
    type == 'Interactive Feature' | type == 'Video'
    ) %>% 
  count()

# as expected, it is the same
```
Out of these different news categories, 454 'articles' did not have any main text, because they only had a headline. This was the case for the interactive features and the videos. 

Looking into the article types actually containing text. 

```{r}
# for all subsections
d %>% 
  filter(type == 'Obituary (Obit)' | type == 'briefing' | type == 'News Analysis'
         ) %>% 
  count()
# only 67 out of 3036

d %>% 
  filter(type == 'News'
         ) %>% 
  count()
```
*Summary*
Just short of 3500 articles , roughly 500 of these are videos/interactive features, which mean they only have a headline. 
Thus, the plots only concerning the main text have around 3000 data points to play around with. 
Most articles are labelled as 'news'. 


# making one mean per article (mean of text and headline sentiment compound score)
```{r}
# function
get_mean_sent_comp <- function(row_no){
  
  mean_sent <- ifelse(d$word_count[row_no] != 0, 
                      (d$headline_sentiment_comp[row_no] + d$text_sentiment_comp[row_no])/2,
                      d$headline_sentiment_comp[row_no]) %>% data.frame
 # mean_sent = mean(d$headline_sentiment_comp[row_no] + d$text_sentiment_comp[row_no])
#  rowMeans(d[row_no,c('headline_sentiment_comp', 'text_sentiment_comp')], na.rm=TRUE),
  return(mean_sent)
}

# loop through the page numbers and append to one dataframe using map_dfr (r =  by rows)
# first data batch
d_mean_sent <- map_dfr(1:3490, get_mean_sent_comp)
```


```{r}
# binding dataframes
d <- cbind(d, d_mean_sent)
```

```{r}
# fixing column name
d <- d %>%
  rename(
    mean_sent_comp = "."
)
```

```{r}
# testing that it worked 
row_no = 1

d$headline_sentiment_comp[row_no]
d$text_sentiment_comp[row_no]
d$mean_sent[row_no]

# it works
```


# Plotting 

```{r}
# manipulate dataframe, only containing sentiment compound score
df_long_2 <- d %>% 
  select(-c(
    headline_sentiment_but, text_sentiment_but
    )) %>% 
  pivot_longer(
    cols = contains('comp'),
    names_to = 'sent_type',
    values_to = 'sent_score'
  )
```

# Main plot

```{r}
# compound sentiment score per subsection 
df_long_2  %>% 
  group_by(sent_type) %>% 
  ggplot() +
  aes(x = subsection, y = sent_score, color = sent_type) +
  stat_summary(fun = 'mean', geom = 'point', size = 1) +
  stat_summary(fun.data = "mean_se", geom = 'errorbar', width = .1) +
  labs(
    title = 'Sentiment compound score of text and headline of articles',
    subtitle = 'Articles from New York Times (1st of January - 15th of September 2021)',
    x = '', 
    y = 'Compound score',
    color = '') +
  scale_color_manual(labels = c('Headline','Mean', 'Text'), values = c('red', 'grey40','blue')) +
  geom_hline(
    yintercept = -.21, 
    alpha = .4, linetype = 'dashed'
  ) + 
  coord_cartesian(ylim = c(-1,1)) #+
  #geom_text(aes(x = factor('Middle East'), y = -.10, label = 'mean'), size = 3)
  
```

# Calculating means
```{r}
#mean of headline
headline_mean <- df_long_2 %>% filter(sent_type == 'headline_sentiment_comp') %>% select(sent_score) 
mean(headline_mean$sent_score) # -0.17
#mean(d$headline_sentiment_comp)

# mean of text
text_mean <- df_long_2 %>% filter(sent_type == 'text_sentiment_comp') %>% select(sent_score) 
mean(text_mean$sent_score) # -0.24

#overall mean
mean(df_long_2$sent_score)
# -0.21


```

# Number of articles per subsection plot
```{r}
# Plotting the number of articles per section
d %>% 
  filter(
    type == 'News' | type == 'Obituary (Obit)' | type == 'briefing' | type == 'News Analysis'
  ) %>% 
  ggplot() +
  aes(x = subsection, fill = subsection) +
  geom_bar() +
  theme(legend.position = 'NULL') + 
  labs(
    title = 'Number of articles per subsection (n total = 3036)',
    subtitle = 'Articles from New York Times (1st of January - 15th of September 2021)',
    x = '',
    y = 'Number of articles'
  ) +
  geom_text(stat = 'count', aes(label = ..count..), vjust = +2) #vjust changes the position of the count 
```

# Development over time plots
```{r}
# development over time plot - would make more sense with more data and maybe more news media
# creating test df
d_test <- d

# making a new column which only specifies the month
d_test$month <- month(d_test$pub_date)

# get the mean sentiment compound score for the texts per subsection per month
d_mean <- d_test %>% 
  group_by(
    month, 
    subsection) %>% 
  summarize(
    mean_sent = mean(text_sentiment_comp)
    )

# creating labels 
months_labs <- c('January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September')

#plotting
d_mean %>% 
  ggplot() +
  aes(
    as.factor(month), mean_sent, color = subsection, group = subsection
  ) + 
geom_line() + 
  scale_x_discrete(
    breaks = c(1, 2,3,4,5,6,7,8,9),
    labels = months_labs) +
  labs(
    title = 'Average sentiment compound score of text per month per subsection',
    subtitle = 'Articles from New York Times (1st of January - 15th of September 2021)',
    x = '',
    y = 'Average sentiment compound score of text'
  ) + 
  theme(
    legend.title = element_blank()
    ) 

#+ 
  facet_wrap(~subsection, ncol = 2)
```
This makes is necessary to look at the number of articles per subsection per month. 

```{r}
# count number of articles per subsection per month
d_mean_no <- d_test %>% 
  group_by(
    month, 
    subsection) %>% 
  count()
```


```{r}
#plotting number of articles per subsection per month
d_mean_no %>% 
  ggplot() +
  aes(
    as.factor(month), n, color = subsection, group = subsection
  ) + 
geom_line() + 
  scale_x_discrete(
    breaks = c(1, 2,3,4,5,6,7,8,9),
    labels = months_labs) +
  labs(
    title = 'Number of articles per subsection per month',
    subtitle = 'Articles from New York Times (1st of January - 15th of September 2021)',
    x = '',
    y = 'Number of articles'
  ) + 
  theme(
    legend.title = element_blank()
    ) 
```

Not any clear trends, but this might be due to the number of data points in each subsection per month

```{r}
# counting number of articles about Africa in January
d_test %>% filter(
  subsection == 'Africa',
  month == '1'
) %>% 
  count()

# counting number of articles about Africa in general 
d %>% filter(
  subsection == 'Africa',
  type == 'News' | type == 'Obituary (Obit)' |type == 'briefing' | type == 'News Analysis',
) %>% 
  count()
```
E.g., 19 articles were published about Africa in January 2021. So, we don't have a lot of data points per subsection per month. To get more data we would need to compare more news media.  





# Other plots, not as nice

```{r}
# bad plot - starting of somewehre
d %>% ggplot() +
  aes(no, headline_sentiment_comp, color = subsection) +
  geom_point()

# headline sentiment compound score
d %>% filter(
  !subsection == 'What in the World'
  ) %>% 
  ggplot() +
  aes(subsection, headline_sentiment_comp) +
  stat_summary(fun = mean, geom = 'point') + 
  stat_summary(fun.data = "mean_se", geom = 'errorbar', width = .1) +
  labs(
    title = 'Sentiment compound score of the headlines of articles',
    subtitle = 'Articles from New York Times (1st of January - 15th of September 2021)',
    x = '', 
    y = 'Compound score')

# text sentiment compound score
d %>% filter(
  !subsection == 'What in the World',
  type == 'News' | type == 'Orbituary (Obit)' | type == 'briefing' | type == 'News Analysis' # removing videos and Interactive Feature
  ) %>% 
  ggplot() +
  aes(subsection, text_sentiment_comp) +
  stat_summary(fun = mean, geom = 'point') + 
  stat_summary(fun.data = "mean_se", geom = 'errorbar', width = .1) +
  labs(
    title = 'Sentiment compound score of the main text of articles',
    subtitle = 'Articles from New York Times (1st of January - 15th of September 2021)',
    x = '', 
    y = 'Compound score')
```

```{r}
# manipulate dataframe for better plotting
df_long <- d %>% 
  select(-c(
    headline_sentiment_but, text_sentiment_but
    )) %>% 
  pivot_longer(
    cols = contains('sentiment'),
    names_to = 'sent_type',
    values_to = 'sent_score'
  )

# removing but count, as it messes things up and is irrelevant
```

```{r}

# plot the different sentiment scores per subsection
# works
# with colours
df_long %>% 
  filter(
    !subsection == 'What in the World'
  ) %>% 
  group_by(sent_type) %>% 
  ggplot() +
  aes(x = subsection, y = sent_score, color = sent_type) +
  stat_summary(fun = 'mean', geom = 'point', size = 3) 

# with shapes
df_long %>% 
  filter(
    !subsection == 'What in the World'
  ) %>% 
  group_by(sent_type) %>% 
  ggplot() +
  aes(x = subsection, y = sent_score, shape = sent_type) +
  stat_summary(fun = 'mean', geom = 'point', size = 3) +
  scale_shape_manual(values = 7:nlevels(df_long$sent_type))

# faect wrapped version
df_long %>% 
  filter(
    !subsection == 'What in the World'
  ) %>% 
  group_by(sent_score) %>% 
  ggplot() +
  aes(x = subsection, y = sent_score, color = sent_type) +
  stat_summary(fun = 'mean', geom = 'point') +
  facet_wrap(~sent_type, ncol= 2
  ) + theme(legend.position = "none")

# bad plot
df_long %>% 
  group_by(sent_type) %>% 
  ggplot() +
  aes(x= subsection, y=sent_score, group = sent_type) +
    stat_summary(fun = 'mean', geom = 'point', size = 4, shape = 21, fill = 'white') +
  geom_line(aes(group = sent_type))
#+
  stat_summary(fun = 'mean', geom = 'line', color = 'green', aes(group = 1))

```



```{r}
# plot the different sentiment scores per subsection
# works - good plot
# with colours
df_long_2  %>% 
  group_by(sent_type) %>% 
  ggplot() +
  aes(x = subsection, y = sent_score, color = sent_type) +
  geom_rect(data = NULL, aes(
    xmin = -Inf, xmax = Inf, ymin = -1, ymax = -0.05
    ), fill = 'tomato', color = 'tomato', alpha = .01, show.legend = F
  ) + 
  geom_rect(data = NULL, aes(
    xmin = -Inf, xmax = Inf, ymin = .05, ymax = 1
  ), fill = 'palegreen', color = 'palegreen', alpha = .01, show.legend = F
  ) +
  geom_rect(data = NULL, aes(
    xmin = -Inf,xmax = Inf, ymin = -.05, ymax = .05), 
    fill = 'oldlace', color = 'oldlace', alpha = .1, show.legend = F
  ) +
  stat_summary(fun = 'mean', geom = 'point', size = 2) +
  stat_summary(fun.data = "mean_se", geom = 'errorbar', width = .1) +
  labs(
    title = 'Sentiment compound score of the text and the headline of articles',
    subtitle = 'Articles from New York Times (1st of January - 15th of September 2021)',
    x = '', 
    y = 'Compound score',
    color = '') + 
  scale_color_manual(labels = c('Headline', 'Text'), values = c('red', 'blue')) +
  geom_hline(
    yintercept = -.21, 
    alpha = .4
  ) + 
  coord_cartesian(ylim = c(-1,1)) + 
  scale_color_manual(values = c('springgreen', 'navy'))
  
   #geom_rect(data=NULL,aes(xmin=0.25,xmax=7.25,ymin=-Inf,ymax=Inf),
   #                 fill="lightgreen")


#+ 
  #geom_text(aes(x = factor('Middle East'), y = .2, label = 'mean')
  coord_cartesian(ylim = c(0,1150))


```


```{r}
# creating a dataframe with the wordcount != 0
d_mean <- d %>% 
  filter(
    type == 'News' | type == 'Orbituary (Obit)' | type == 'briefing' | type == 'News Analysis' 
  )
```


```{r}
# number of words per article on average
d %>% 
  filter(
    !subsection == 'What in the World',  
    type == 'News' | type == 'Orbituary (Obit)' | type == 'briefing' | type == 'News Analysis'
) %>% 
  ggplot() +
  aes(subsection, word_count, color = subsection) + 
  stat_summary(fun = 'mean', geom = 'point') +
  stat_summary(fun.data = 'mean_se', geom = 'errorbar', width = .1) +
  labs(
    title = 'Mean word count of articles (text) per subsection with standard error bars',
    subtitle = 'Articles from New York Times (1st of January - 15th of September 2021)',
    x = '',
    y = 'word count'
  ) + 
  theme(legend.position = 'none') +
  geom_hline(yintercept = mean(d_mean$word_count), alpha = .4) +
  coord_cartesian(ylim = c(0,1150))

```

The subsections have roughly the same number of words per article, which makes sense since they probably have a set limit of words per article. 


# Modelling time 


```{r models}
m1 <- lm(mean_sent_comp ~ subsection, d)
summary(m1)
```

