---
title: "final_project_NYT"
author: "Sigrid Agersnap Bom Nielsen"
date: "11/3/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
pacman::p_load(tidyverse, jsonlite, rvest, vader)
```



```{r}
url <-  'https://api.nytimes.com/svc/search/v2/articlesearch.json?begin_date=20210101&end_date=20210501&fq=fq%20%3D%20section_name%3A(%22World%22)%20AND%20subsection_name%3A(*)&api-key=ZAJ6xiTXYbfqkdUJqQKGoyPKXhbosYQC'
```

```{r}
# looking at the data using jsonlite 
d <- fromJSON(url) %>% data.frame()
test <- fromJSON(url)

```

We have what we need, both the 'World' section and a separate column with the names of the subsection. Plus, of course the individual link to each article. 

*only 10 articles however* - there are only 10 articles per page. we need to loop through the pages to get all of the articles. 

For the first round of data I only hace 1546 hits. This gives 

```{r}
# get total amount of pages
pages <- round(d$response.meta.hits/10-1)[1]
# the first page is 0 so that's why

```
roughly 154 pages!


```{r}
# function which takes one page number as input and
get_article <- function(page_no){
  articles <- fromJSON(paste0(url, "&page=", page_no)) %>% data.frame
  Sys.sleep(6)
  return(articles)
}

# loop through the page numbers and append to one dataframe using map_dfr (r =  by rows)
full_df <- map_dfr(0:pages, get_article)

# get an idea of the data and the classes of the data
glimpse(df)

#deselect difficult matrix like columns
df <- full_df %>% 
  select(-c(
    'response.docs.multimedia', 
    'response.docs.keywords', 
    'response.docs.byline',
    'response.docs.headline')
    )

# save data as csv file
write_csv(df, 'data_url.csv', na = 'NA')
```

# Next step, get the text from each url.
First, use rvest to get the text (headline + body) from one url - make that as a function.
Then, map it onto all of the urls. 

```{r}
#getting URL 
test <- read_html(df$response.docs.web_url[1])

# text headline - first try
 test %>% html_elements('.css-ymxi58, .e1h9rw200') %>% 
  html_text()

# text headline - better option
test_headline <- test %>% html_element('[data-testid="headline"]') %>% 
  html_text()

# text body !
test_text <- test %>% html_elements('.meteredContent') %>% 
  html_text()

test_headline = ifelse(length(test_headline) == 0, 'NA', test_headline)
test_text = ifelse(length(test_text) == 0, 'NA', test_text)

d_test <- data.frame(test_headline, test_text)


# testing on 1 row only  - function
get_text <- function(n){
  link = read_html(df$response.docs.web_url[n])
  
  headline = link %>% html_element('[data-testid="headline"]') %>% 
  html_text()
  
  text = link %>% html_elements('.meteredContent') %>% 
  html_text()
  
  headline = ifelse(length(headline) == 0, 'NA', headline)
  text = ifelse(length(text) == 0, 'NA', text)
  
  d = data.frame(headline, text)
  
  Sys.sleep(6)
  
  message = 'Retrieving article no'
  print(paste(message, n))
  
  return(d)
}

# it works!!
test_3 <- get_text(3)
test2 <- get_text(df$response.docs.web_url[3])
test2
test_3


# this also works!
df_text <- map_dfr(1:5, get_text)

# merge when done - works
#df_global <- merge(df_text, df)


# make the big dataframe
df_text <- map_dfr(1:1546, get_text)

write_csv(df_text, 'df_text.csv')


# check for missing values
which(is.na(df_text) == T)

#none!
```

# Time to get to the sentiment analysis 

```{r}
# testing out who the vader package works
get_vader(test_3$text)

sen_list <- get_vader(test_3$text)

sen_list[2]

sen_score <- length(get_vader(test_3$text)

# get sentiment for one row only
headline_sentiment_test <- get_vader(df_text$headline[1])

get_vader(df_text$headline[1])
headline_sentiment_compound_test <- headline_sentiment_test[2]
```


```{r}
#Make a sentiment analysis on one row only - function

get_sentiment <- function(n){
  # get the sentiment of the headline and save it into different variables
  headline_sentiment = get_vader(df_text$headline[n])
  headline_sentiment_comp = headline_sentiment[2]
  headline_sentiment_pos = headline_sentiment[3]
  headline_sentiment_neu = headline_sentiment[4]
  headline_sentiment_neg = headline_sentiment[5]
  headline_sentiment_but = headline_sentiment[6]
  
  # get the sentiment of the text and save it into different variables
  text_sentiment = get_vader(df_text$text[n])
  text_sentiment_comp = text_sentiment[2]
  text_sentiment_pos = text_sentiment[3]
  text_sentiment_neu = text_sentiment[4]
  text_sentiment_neg = text_sentiment[5]
  text_sentiment_but = text_sentiment[6]
  
  # printing a message to follow the process
  message = 'Retrieving article no'
  print(paste(message, n))
  
  # gather the variables in one dataframe
  df_sent = 
    data.frame(
    headline_sentiment_comp,
    headline_sentiment_pos,
    headline_sentiment_neu,
    headline_sentiment_neg,
    headline_sentiment_but,
    text_sentiment_comp,
    text_sentiment_pos,
    text_sentiment_neu,
    text_sentiment_neg,
    text_sentiment_but, 
    row.names = n # fixing row names to be 1:1546
  )
  
  return(df_sent)
}

#test one one row only - works
sent_test = get_sentiment(1)

# real deal - will prabably take about 3 hours to run
df_sentiment <- map_dfr(1:1546, get_sentiment)

# merge some of df and df_text(maybe not?) and df_sentiment
```
# Sentiment analysis on each section
Read about Vader - how does it work, what are the possibilities within the package, which approach does it have to grading words, what are its flaws etc. 
Can I simply use mean() to get a mean sentiment compound score per world section? 

# Plotting time

# make presentation - Tuesday the 9th ? 
# make video before the 22nd of November
# write report 

# make nice knitted document with floating titles etc. (see homework)
# find the link which Jonathan posted on the slack channel 